{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df0ee9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: lxml in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (5.4.0)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from bs4) (4.13.4)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Downloading numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.5 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4 lxml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e13af196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa2e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of websites: 15\n",
      "['airbnb', 'aliexpress', 'ebay', 'facebook', 'github', 'linkedin', 'medium', 'netflix', 'pinterest', 'quora', 'reddit', 'twitch', 'twitter', 'walmart', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "websites_path = \"./../dataset/websites.json\"\n",
    "\n",
    "with open(\"./../dataset/websites.json\", encoding=\"utf-8\") as f:\n",
    "    all_websites = sorted(json.load(f).keys())\n",
    "\n",
    "print(\"Number of websites:\", len(all_websites))\n",
    "print(all_websites)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "418f4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dom_tree_height(dom: str):\n",
    "    html = open(f'./../dataset/original/{dom}.html', encoding='utf-8').read()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    def max_depth_bs(node, current=0):\n",
    "        current += 1\n",
    "        depths = [current]\n",
    "\n",
    "        for child in node.contents:\n",
    "            if getattr(child, 'contents', None):\n",
    "                depths.append(max_depth_bs(child, current))\n",
    "        return max(depths)\n",
    "\n",
    "    max_depth = max_depth_bs(soup)\n",
    "    print(f\"Height of the {dom} HTML Tree:\", max_depth)\n",
    "    return max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd03d910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height of the airbnb HTML Tree: 32\n",
      "Height of the aliexpress HTML Tree: 24\n",
      "Height of the ebay HTML Tree: 27\n",
      "Height of the facebook HTML Tree: 20\n",
      "Height of the github HTML Tree: 24\n",
      "Height of the linkedin HTML Tree: 17\n",
      "Height of the medium HTML Tree: 17\n",
      "Height of the netflix HTML Tree: 18\n",
      "Height of the pinterest HTML Tree: 5\n",
      "Height of the quora HTML Tree: 8\n",
      "Height of the reddit HTML Tree: 24\n",
      "Height of the twitch HTML Tree: 9\n",
      "Height of the twitter HTML Tree: 4\n",
      "Height of the walmart HTML Tree: 27\n",
      "Height of the youtube HTML Tree: 12\n",
      "Tree heights saved to tree_heights.json\n"
     ]
    }
   ],
   "source": [
    "tree_heights = {}\n",
    "\n",
    "for website in all_websites:\n",
    "    tree_heights[website] = get_dom_tree_height(website)\n",
    "\n",
    "with open(\"./../dataset/tree_heights.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tree_heights, f, ensure_ascii=False, indent=4)\n",
    "print(\"Tree heights saved to tree_heights.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a983e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of tree heights:\n",
      "height\n",
      "4     1\n",
      "5     1\n",
      "8     1\n",
      "9     1\n",
      "12    1\n",
      "17    2\n",
      "18    1\n",
      "20    1\n",
      "24    3\n",
      "27    2\n",
      "32    1\n",
      "Name: count, dtype: int64\n",
      "Mean tree height: 17.866666666666667\n",
      "Median tree height: 18.0\n",
      "Max tree height: 32\n",
      "Min tree height: 4\n"
     ]
    }
   ],
   "source": [
    "# get distribution of tree heights using pandas\n",
    "\n",
    "df = pd.DataFrame.from_dict(tree_heights, orient='index', columns=['height'])\n",
    "\n",
    "print(\"Distribution of tree heights:\")\n",
    "print(df['height'].value_counts().sort_index())\n",
    "print(\"Mean tree height:\", df['height'].mean())\n",
    "print(\"Median tree height:\", df['height'].median())\n",
    "print(\"Max tree height:\", df['height'].max())\n",
    "print(\"Min tree height:\", df['height'].min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
