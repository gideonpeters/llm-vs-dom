{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec53805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gideonpeters/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag, Comment\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "import tiktoken\n",
    "import os\n",
    "from dom_chunker import HTMLChunker\n",
    "# from html_chunking import get_html_chunks, merge_html_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703b702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_path = \"./../dataset/websites.json\"\n",
    "\n",
    "with open(\"./../dataset/websites.json\", encoding=\"utf-8\") as f:\n",
    "    all_websites = sorted(json.load(f).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74849226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing walmart...\n",
      "Processed walmart with 5 chunks.\n",
      "Processing pinterest...\n",
      "Processed pinterest with 4 chunks.\n",
      "Processing linkedin...\n",
      "Processed linkedin with 6 chunks.\n",
      "Processing reddit...\n",
      "Processed reddit with 10 chunks.\n",
      "Processing ebay...\n",
      "Processed ebay with 16 chunks.\n",
      "Processing github...\n",
      "Processed github with 9 chunks.\n",
      "Processing twitter...\n",
      "Processed twitter with 4 chunks.\n",
      "Processing facebook...\n",
      "Processed facebook with 4 chunks.\n",
      "Processing aliexpress...\n",
      "Processed aliexpress with 8 chunks.\n",
      "Processing netflix...\n",
      "Processed netflix with 4 chunks.\n",
      "Processing youtube...\n",
      "Processed youtube with 4 chunks.\n",
      "Processing twitch...\n",
      "Processed twitch with 4 chunks.\n",
      "Processing medium...\n",
      "Processed medium with 4 chunks.\n",
      "Processing quora...\n",
      "Processed quora with 4 chunks.\n",
      "Processing airbnb...\n",
      "Processed airbnb with 10 chunks.\n"
     ]
    }
   ],
   "source": [
    "path_to_dom_trees = \"./../dataset/original/\"\n",
    "\n",
    "def get_html_files(path):\n",
    "    html_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                html_files.append(os.path.join(root, file))\n",
    "    return html_files\n",
    "\n",
    "def get_html_file_name(file_path):\n",
    "    # Get the file name without the directory and extension\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    return file_name\n",
    "\n",
    "\n",
    "all_dom_trees = get_html_files(path_to_dom_trees)\n",
    "\n",
    "def process_html_file(file_path, chunk_size=15000):\n",
    "    file_name = get_html_file_name(file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    chunker = HTMLChunker(html_content, chunk_size)\n",
    "    chunks = chunker.split_html()\n",
    "    chunks = chunker.store_chunks(chunks, chunked_file_directory=\"./../dataset/chunks/\" + file_name)\n",
    "    # modified_html = chunker.reassemble_html(chunks)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_chuks = {}\n",
    "for file_path in all_dom_trees:\n",
    "    file_name = get_html_file_name(file_path)\n",
    "    print(f\"Processing {file_name}...\")\n",
    "    chunks = process_html_file(file_path, chunk_size=15000)\n",
    "    print(f\"Processed {file_name} with {len(chunks)} chunks.\")\n",
    "    tree_chuks[file_name] = len(chunks)\n",
    "\n",
    "\n",
    "with open(\"./../dataset/website_chunks.json\", 'w') as f:\n",
    "    f.write(json.dumps(tree_chuks, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
